{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "## Evaluation functions\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "## Models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# imbalance-learn\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All relevant functions and classes are contained in module `wgan`, which has pytorch as its main dependency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wgan.imblearn import GANbalancer\n",
    "#import wgan.data_loader as data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load the COIL00 dataset from an old competition on customer scoring that contains numeric and categorical variables. The preprocessing is outsourced to a function load_coil00. I assume that the data contains no missing variables and is loaded as a pandas `DataFrame` where all categorical variables have the new pandas type `category` for preprocessing. The balancer object itself will run on a numpy array.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i './data/load_coil00.py'\n",
    "X, y = load_coil00(\"./data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exposition, I subsample 5 continuous and 5 categorical variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_set = list(range(0,5)) + list(range(60,65))\n",
    "X = X.iloc[:,var_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5822, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GANbalancer creates an embedding layer for each categorical variable. To do so, it needs to know the categorical variables and how many classes exist within each variable. This is easier to automize if this information is set in the data object, but we will feed it to the GANbalancer manually, so it is possible to avoid pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize index lists\n",
    "idx_cont = None\n",
    "idx_cat  = None\n",
    "\n",
    "if idx_cat is None:\n",
    "    idx_cat = list(np.where(X.dtypes == 'category')[0])\n",
    "    idx_cat = [int(x) for x in idx_cat]\n",
    "\n",
    "if idx_cont is None:\n",
    "    idx_cont = [x for x in range(X.shape[1]) if x not in idx_cat]\n",
    "    idx_cont = [int(x) for x in idx_cont]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GANbalancer takes as input a list of tuples of the indices of the categorical variables, the number of levels within the variable and the number of embeddings used in the critic. The critic takes the raw output of the generator as input, where each categorical variable is one-hot encoded (soft one-hot encoding for the generator output). The critic then runs each categorical variable through an embedding layer, the size of which we specify here for each variable separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding tuples\n",
    "categorical = None\n",
    "if idx_cat is not None:\n",
    "    categorical = [(i, # Cat. Var. index\n",
    "                    len(X.iloc[:,i].cat.categories), # Number of categories (generator ouput size for this variable)\n",
    "                    # Critic Embedding layer size: I suggest the heuristic no_nodes = min(no. of levels/2, 15)\n",
    "                    int(min(15., np.ceil(0.5*len(X.iloc[:,i].cat.categories))))\n",
    "                   )\n",
    "                    for i in idx_cat]\n",
    "\n",
    "# Make sure categorical variables are encoded from 0\n",
    "if np.any([idx>min(idx_cat) for idx in idx_cont]):\n",
    "    raise ValueError(\"Variables need to be ordered [cont, cat]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After collecting the information on the categorical variables, we can transform the data to a numpy array. Note that categorical variables are level encoded (0,..., no. of levels) rather than one-hot encoded at this point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.to_numpy(dtype=np.float32)\n",
    "y=y.to_numpy(dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the critic contains embedding layers in the input module, the GANbalancer works on label-encoded variables (0,1,2,...) rather than one-hot encoded variables. Since most sklearn classifiers take one-hot encoded input, we'll use two different preprocessing objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for GANbalancer\n",
    "preproc_sampler = ColumnTransformer([\n",
    "    ('scaler', MinMaxScaler(), idx_cont),\n",
    "    ('pass',   'passthrough',  idx_cat)\n",
    "])\n",
    "\n",
    "# Preprocessing for Logistic Regression\n",
    "preproc_clf = ColumnTransformer([\n",
    "    ('pass', 'passthrough', idx_cont),\n",
    "    ('ohe',   OneHotEncoder(categories='auto', handle_unknown='ignore'),  idx_cat)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GANbalancer itself is now simple to construct through a nice wrapper. Defaults are currently fast but not optimal and require more empirical validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = GANbalancer(\n",
    "            sampling_strategy={1:700}, # How many samples to have after sampling? \"auto\" balances to 50:50\n",
    "            idx_cont=idx_cont,  # List of ndices of continuous variables\n",
    "            categorical=categorical, # List of tuples with info on categorical variables (see above)\n",
    "            \n",
    "            verbose = 1,\n",
    "    \n",
    "            auxiliary=True, # Train one conditional generator for all classes\n",
    "            gan_architecture=\"fisher\", # Which GAN loss function to use? Fisher GAN is recommended\n",
    "            generator_input= X.shape[1], # Noise input to the generator\n",
    "            generator_layers=[50,50], # Layer in the Generator, typically one or two hidden layers\n",
    "            critic_layers=[50,50], # Layer in the Critic (w/o embedding layers), typically one or two hidden layers\n",
    "            layer_norm=True, # Layer normalization? Recommended\n",
    "            \n",
    "            batch_size=64, \n",
    "            learning_rate=(5e-05, 5e-05),\n",
    "            n_iter=1e5,  # No. of Generator updates. In the range of [1e5 to 1e6]\n",
    "            critic_iterations=3) # Number of critic updates before each generator update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GANbalancer works with any model from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='liblinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a pipeline for convenience. The GANbalancer is only needed during model training and can be discarded for deployment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "            ('preproc_sampler', preproc_sampler),\n",
    "            ('sampler', sampler),\n",
    "            ('preproc_clf', preproc_clf),\n",
    "            ('classifier', model)\n",
    "          ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of the GANbalancer may require some time depending on the number of updates `n_iter`. 10000 iterations on full COIL00 should take ~20 minutes on an average CPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100010it [1:04:23, 27.76it/s]                             \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('preproc_sampler', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
       "         transformer_weights=None,\n",
       "         transformers=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1)), [0, 1, 2, 3, 4]), ('pass', 'passthrough', [5, 6, 7, 8, 9])])), ('sampler', GANbalancer...ty='l2', random_state=None, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%timeit\n",
    "pipeline.fit(X=X_train,y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a benchmark model without adressing the imbalance in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('preproc_clf', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
       "         transformer_weights=None,\n",
       "         transformers=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1)), [0, 1, 2, 3, 4]), ('ohe', OneHotEncoder(categorical_features=None, categories='auto',\n",
       "    ...ty='l2', random_state=None, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_benchmark = Pipeline(steps=[\n",
    "            ('preproc_clf', ColumnTransformer([\n",
    "                                ('scaler', MinMaxScaler(), idx_cont),\n",
    "                                ('ohe',   OneHotEncoder(categories='auto', handle_unknown='ignore'),  idx_cat)\n",
    "                            ])\n",
    "            ),\n",
    "            ('classifier', LogisticRegression(solver='liblinear'))\n",
    "          ])\n",
    "\n",
    "pipeline_benchmark.fit(X=X_train,y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple comparison of the AUC as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of model trained on training and synthetic data: 0.5774791566963049\n",
      "AUC of model trained only on imbalanced training data : 0.6300722903705196\n"
     ]
    }
   ],
   "source": [
    "print(\"AUC of model trained on training and synthetic data:\",    roc_auc_score(y_true=y_test, y_score=pipeline.predict_proba(X_test)[:,1]))\n",
    "print(\"AUC of model trained only on imbalanced training data :\", roc_auc_score(y_true=y_test, y_score=pipeline_benchmark.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating synthetic data manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = pipeline.named_steps[\"sampler\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_data = gan.generator.sample_data(num_samples=10000, class_index=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GANbalancer creates continuous variables on the normalized scale (after the first preprocessing step in the pipepline). Categorical variable levels are sampled and returned as a single column of labels to match the original data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_data = pipeline.named_steps[\"preproc_clf\"].transform(syn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 39)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-0.5809464 , -0.15651135,  0.7383057 ,  0.96522825,  1.085758  ,\n",
       "          0.        ,  0.6354    ,  0.361     ,  0.0036    ,  0.        ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn_data.mean(axis=0)[:,0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.01168117, 0.42046496, 0.07716191, 0.51613479, 0.21516262,\n",
       "         0.01259734, 0.2478241 , 0.51855245, 0.18140174, 0.03481448]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_benchmark.named_steps[\"preproc_clf\"].transform(X_train).mean(axis=0)[:,0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [deeplearning]",
   "language": "python",
   "name": "Python [deeplearning]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
