{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MKL_NUM_THREADS=1\n",
      "env: NUMEXPR_NUM_THREADS=1\n",
      "env: OMP_NUM_THREADS=1\n",
      "env: OPENBLAS_NUM_THREADS=1\n",
      "env: VECLIB_MAXIMUM_THREADS=1\n"
     ]
    }
   ],
   "source": [
    "%env MKL_NUM_THREADS=1\n",
    "%env NUMEXPR_NUM_THREADS=1\n",
    "%env OMP_NUM_THREADS=1\n",
    "%env OPENBLAS_NUM_THREADS=1\n",
    "%env VECLIB_MAXIMUM_THREADS=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation functions\n",
    "from sklearn.metrics import roc_auc_score, make_scorer\n",
    "from lift.perc_lift_score import perc_lift_score\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Samplers\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from wgan.imblearn import GANbalancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X,y = make_classification(n_samples=4000, n_features=20, weights=[0.99,0.01], \n",
    "                          n_informative=20, n_redundant=0, n_clusters_per_class=5)\n",
    "\n",
    "# from wgan.data import load_DMC10\n",
    "# X, _, y, _, idx_cont, idx_cat, cat_dict = load_DMC10(\"/Users/hauptjoh/Data/DMC10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment.experiment_config import experiment_config\n",
    "scorers, models, samplers = experiment_config(X, idx_cont=None, idx_cat=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_cont=None\n",
    "idx_cat = None\n",
    "\n",
    "### Samplers\n",
    "scorers = {'auc':make_scorer(roc_auc_score, needs_proba=True),\n",
    "          'TDLift':make_scorer(perc_lift_score, needs_proba=True, percentile=0.1)}\n",
    "\n",
    "### Models\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression(solver='liblinear'), {\n",
    "    \"C\": [10]\n",
    "}))\n",
    "# models.append(('RF', RandomForestClassifier(), {\n",
    "#     \"n_estimators\":[100],\n",
    "#     \"max_features\":[\"sqrt\"],\n",
    "#     \"min_samples_leaf\":[20]\n",
    "# }))\n",
    "\n",
    "### Samplers\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from wgan.imblearn import GANbalancer\n",
    "\n",
    "samplers = []\n",
    "\n",
    "# GAN\n",
    "if idx_cont is None:\n",
    "    idx_cont = list(range(X.shape[1]))\n",
    "\n",
    "categorical = None\n",
    "if idx_cat is not None:\n",
    "    categorical = [(i,\n",
    "                    np.max(X[:,i])+1,\n",
    "                    int(min(15., np.ceil(np.max((X[:,i])+1)/2)))\n",
    "                   )\n",
    "                    for i in idx_cat]\n",
    "\n",
    "samplers.append(('cGAN', GANbalancer(\n",
    "        idx_cont=idx_cont, categorical=categorical, batch_size = 128, critic_iterations=1\n",
    "), {\n",
    "    'generator_input'  : [50],\n",
    "    'generator_layers' : [[70]],\n",
    "    'critic_layers'    : [[40]],\n",
    "    'n_iter'           : [40000,100000,1000000]\n",
    "}))\n",
    "\n",
    "# baseline\n",
    "samplers.append(('unbalanced', None, {}))\n",
    "\n",
    "# SMOTE\n",
    "samplers.append(('SMOTE', SMOTE(), {\n",
    "    'k_neighbors':[5,10]\n",
    "}))\n",
    "\n",
    "# ADASYN\n",
    "samplers.append(('ADASYN', ADASYN(), {\n",
    "    'n_neighbors':[5,10]\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "seed = 123\n",
    "\n",
    "score_outer = {}\n",
    "\n",
    "for sampler_name, sampler, sampler_grid in tqdm(samplers):\n",
    "    \n",
    "    sampler_grid = {'sampler__'+key:item for key, item in sampler_grid.items()}\n",
    "    \n",
    "    score_inner = {}\n",
    "\n",
    "    for model_name, model, model_grid in tqdm(models):\n",
    "\n",
    "        pipeline = Pipeline(memory='./.cachedir', steps=[\n",
    "            ('scaler', MinMaxScaler()),\n",
    "            ('sampler', sampler),\n",
    "            ('classifier', model)\n",
    "          ])\n",
    "        model_grid = {'classifier__'+key:item for key, item in model_grid.items()}\n",
    "        p_grid = {**sampler_grid, **model_grid}\n",
    "        \n",
    "        inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "        outer_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "            \n",
    "        clf = GridSearchCV(pipeline, param_grid= p_grid, cv=inner_cv, scoring=scorers, refit='auc', \n",
    "                           return_train_score=True, iid=False, n_jobs=20, pre_dispatch=40, verbose=1)\n",
    "\n",
    "        score_inner[model_name] = cross_validate(clf, X=X,y=y,cv=outer_cv , scoring=scorers, return_train_score=True,\n",
    "                                    return_estimator=True, verbose=1)\n",
    "    score_outer[sampler_name] = score_inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame([{\n",
    "  'sampler':sampler_name, 'model':model_name, \n",
    "    'auc':np.mean(model[\"test_auc\"]),  'auc_sd':np.std(model[\"test_auc\"]),\n",
    "    'lift0.1':np.mean(model[\"test_TDLift\"]),  'lift0.1_sd':np.std(model[\"test_TDLift\"]),\n",
    "} for sampler_name, sampler in score_outer.items()\n",
    "    for model_name, model in sampler.items()]\n",
    ")\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_results = {sampler_name:\n",
    "    {model_name:\n",
    "    # vstack result DataFrame for each outer fold\n",
    "        pd.concat([ \n",
    "            # Inner CV tuning results as DataFrame\n",
    "            pd.concat([pd.DataFrame(inner_cv.cv_results_['params']).astype(str), \n",
    "                       pd.DataFrame({\n",
    "                           'mean_test_auc':inner_cv.cv_results_['mean_test_auc'],\n",
    "                           'std_test_auc':inner_cv.cv_results_['std_test_auc'],\n",
    "                           'mean_test_TDLift':inner_cv.cv_results_['mean_test_TDLift'],\n",
    "                           'std_test_TDLift':inner_cv.cv_results_['std_test_TDLift']\n",
    "                       })\n",
    "                      ], sort=False, ignore_index=False, axis=1)\n",
    "            for inner_cv in model['estimator']]).groupby(list(model['estimator'][0].cv_results_['params'][0].keys())).mean().reset_index()\n",
    "            for model_name, model in sampler.items()}\n",
    "          for sampler_name, sampler in score_outer.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_results[\"cGAN\"][\"LR\"].sort_values([\"sampler__n_iter\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_scores =  pd.DataFrame([{\n",
    "  'sampler':sampler_name, 'model':model_name,\n",
    "    'parameter':param_name,\n",
    "    'parameter_value':str(param_value),\n",
    "    'auc':cv.cv_results_['mean_test_auc'][i],  'auc_sd':cv.cv_results_[\"std_test_auc\"][i],\n",
    "    'lift0.1':cv.cv_results_[\"mean_test_TDLift\"][i],  'lift0.1_sd':cv.cv_results_[\"std_test_TDLift\"][i]\n",
    "}   for sampler_name, sampler in score_outer.items()\n",
    "    for model_name, model in sampler.items()\n",
    "    for cv in model['estimator']\n",
    "    for i, (param_name, param_value) in enumerate(cv.cv_results_['params'][0].items())\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_scores.groupby(['sampler','model','parameter','parameter_value']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
