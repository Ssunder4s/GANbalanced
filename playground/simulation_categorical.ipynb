{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import minmax_scale, scale, MinMaxScaler, KBinsDiscretizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wgan.models_cat import Generator, Critic\n",
    "from wgan.training import WGAN\n",
    "from wgan.dataloaders import TabularDataset\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, brier_score_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from imbalanced_sampler.sampler import ImbalancedDatasetSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_correlation_matrix(no_var):\n",
    "    corr = np.zeros([no_var,no_var])\n",
    "    corr_temp = np.random.uniform(-1,1,size=[(no_var-1)*2])\n",
    "    corr[np.triu_indices(no_var, 1)] = corr_temp\n",
    "    corr + corr.T + np.eye(no_var)\n",
    "    return corr\n",
    "\n",
    "\n",
    "def create_continuous_data(N, pos_ratio=0, noise_ratio=0, no_var=10, cov=None, random_state=None):\n",
    "    if random_state is not None: np.random.seed(random_state)\n",
    "    # Group indicator\n",
    "    #group = sp.binom.rvs(p=0.25, n=1, size=N    \n",
    "    N_neg = int(N*(1-pos_ratio))\n",
    "    N_pos = N-N_neg\n",
    "    y = np.concatenate([np.zeros(N_neg), np.ones(N_pos)])\n",
    "    \n",
    "    mean = np.random.uniform(size=no_var)\n",
    "    mean0 = np.random.normal(loc=mean,scale=0.5)\n",
    "    mean1 = np.random.normal(loc=mean,scale=0.5)\n",
    "    \n",
    "    if cov is None: \n",
    "        cov0 = sp.invwishart.rvs(df=no_var*2, scale=np.eye(no_var))\n",
    "        cov1 = sp.invwishart.rvs(df=no_var*2, scale=np.eye(no_var))\n",
    "\n",
    "    # Noise are variables with same distribution in majority and minority class\n",
    "    if noise_ratio != 0:  \n",
    "        no_noise = int(noise_ratio*no_var)\n",
    "        no_var = no_var - no_noise\n",
    "        X_noise = sp.multivariate_normal.rvs(mean=mean0[no_var:], cov=cov0[no_var:,no_var:], size=N).reshape([N,-1])\n",
    "\n",
    "    X1 = sp.multivariate_normal.rvs(mean=mean1[0:no_var], cov= cov1[:no_var,:no_var], size=N_pos)\n",
    "    X0 = sp.multivariate_normal.rvs(mean=mean0[0:no_var], cov= cov0[:no_var,:no_var], size=N_neg)\n",
    "    X = np.vstack([X0,X1])\n",
    "    X = np.hstack([X, X_noise])\n",
    "    \n",
    "    return {\"X\":X, \"y\":y,\"mean0\":mean0,\"mean1\":mean1, \"cov0\":cov0, \"cov1\":cov1}\n",
    "\n",
    "def create_dataset(n_samples=1000, n_features=2, n_classes=3, weights=(0.01, 0.01, 0.98),\n",
    "                   class_sep=0.8, n_clusters=1, random_state=0):\n",
    "    return make_classification(n_samples=n_samples,\n",
    "                               n_informative=2, n_redundant=0, n_repeated=0,\n",
    "                               n_classes=n_classes, n_features = n_features,\n",
    "                               n_clusters_per_class=n_clusters,\n",
    "                               weights=list(weights),\n",
    "                               class_sep=class_sep, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artifical Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modus = 'minority' #'full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_cont = 4\n",
    "no_cat = 4\n",
    "no_vars = no_cont + no_cat\n",
    "N= 50000\n",
    "\n",
    "# Create single dataset to avoid random effects\n",
    "# Only works for all informative features\n",
    "X_full,y = make_classification(n_samples=N, weights=[0.9,0.1], n_clusters_per_class=1,\n",
    "                              n_features=no_vars, \n",
    "                              n_informative=no_vars, \n",
    "                              n_redundant=0, n_repeated=0,\n",
    "                             random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer([\n",
    "    (\"scaler\", MinMaxScaler(), slice(no_cont)),\n",
    "    (\"discretizer\", KBinsDiscretizer(n_bins=5, encode='ordinal', strategy=\"quantile\"),\n",
    "     slice(no_cont,))\n",
    "], remainder=\"drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y, \n",
    "                                                    stratify=y, test_size=0.5, random_state=123)\n",
    "\n",
    "\n",
    "X_train = ct.fit_transform(X_train)\n",
    "X_test = ct.transform(X_test)\n",
    "#scaler = MinMaxScaler()\n",
    "#X_train = scaler.fit_transform(X_train)\n",
    "#X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train_majority = X_train[y_train==0,:]\n",
    "X_train_minority = X_train[y_train==1,:]\n",
    "\n",
    "y_train_bin = y_train[:]\n",
    "y_test_bin = y_test[:]\n",
    "y_temp = np.zeros([len(y_train),2])\n",
    "y_temp[y_train==0,0] = 1\n",
    "y_temp[y_train==1,1] = 1\n",
    "y_train = y_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_levels = [len(np.unique(X_train[:,i])) for i in range(no_cont, no_vars)]\n",
    "\n",
    "#mean_minority = np.mean(X_minority, axis=0)\n",
    "#sd_minority = np.std(X_minority, axis=0)\n",
    "#X_minority = (X_minority-mean_minority)/sd_minority\n",
    "\n",
    "if modus == 'minority':\n",
    "    data_train = TabularDataset(X = X_train_minority[:,:no_cont],\n",
    "                             X_cat = X_train_minority[:,no_cont:],\n",
    "                             y = y_train[np.argmax(y_train, axis=1),:],\n",
    "                                cat_levels = cat_levels\n",
    "                               )\n",
    "elif modus == 'full':\n",
    "    data_train = TabularDataset(X = X_train[:,:no_cont],\n",
    "                             X_cat=X_train[:,no_cont:], \n",
    "                             y=y_train)\n",
    "    data_test = TabularDataset(X = X_test[:,:no_cont],\n",
    "                             X_cat=X_test[:,no_cont:], \n",
    "                             y=y_test)\n",
    "else:\n",
    "    stop(\"Check modus. Must be one of ['minority, 'full]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 5\n",
    "cat_inputs = list(zip(cat_levels, [emb_size] * len(cat_levels)))\n",
    "cont_inputs = no_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat_levels=0\n",
    "#cat_inputs=0\n",
    "#data_train = TabularDataset(X = X_train[:,:no_cont],\n",
    "#                         X_cat=None, \n",
    "#                         y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(latent_dim=10, lin_layer_sizes=[64,128], \n",
    "                      output_dim=cont_inputs, cat_output_dim=cat_levels, aux_dim=0) #[10,10]\n",
    "\n",
    "critic = Critic(input_size=cont_inputs, lin_layer_sizes=[64,128,128], \n",
    "                              cat_input_sizes=cat_inputs, \n",
    "                aux_input_size=0) #[(10,1),(10,1)]\n",
    "\n",
    "print(generator)\n",
    "print(critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_loader = DataLoader(data_train, batch_size = batch_size, shuffle=True)\n",
    "#test_loader = DataLoader(data_test, batch_size = batch_size, shuffle=False)\n",
    "\n",
    "# Balanced sampling through inverse propensiImbalancedDatasetSampler(labels = list(y_train), num_samples=batch_size)ty\n",
    "#data_loader = DataLoader(dataset, batch_size = batch_size, \n",
    "#                     sampler = sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizers\n",
    "lr_G = 5e-5\n",
    "lr_D = 5e-5\n",
    "betas = (.9, .99)\n",
    "G_optimizer = optim.Adam(generator.parameters(), lr=lr_G, betas=betas)\n",
    "C_optimizer = optim.Adam(critic.parameters(), lr=lr_D, betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = WGAN(generator, critic, G_optimizer, C_optimizer, print_every=1000,\n",
    "                  use_cuda=torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.gp_weight = 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(train_loader, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.training_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = f\"cont_cat_n{N//1000}_k{no_vars}_{modus}\"\n",
    "torch.save(generator.state_dict(), f\"../models/wgan_generator_{desc}_{generator.training_iterations}\")\n",
    "torch.save(critic.state_dict(), f\"../models/wgan_critic_{desc}_{generator.training_iterations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"cont_cat_n50_k8_minority_95191\"\n",
    "generator.load_state_dict(torch.load(f\"../models/wgan_generator_{file_name}\"))\n",
    "critic.load_state_dict(torch.load(f\"../models/wgan_critic_{file_name}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = [(x,y) for x in range(no_vars) for y in range(no_vars) if y>x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=no_vars, ncols=no_vars, sharex=True, sharey=True, squeeze=True,figsize=(10,10))\n",
    "for y in axes:\n",
    "    for x in y:\n",
    "        x.set_xticklabels([])\n",
    "        x.set_yticklabels([])\n",
    "\n",
    "for i,j in combinations:\n",
    "    sns.kdeplot(X_majority[:,i], X_majority[:,j], alpha=0.5, cmap=\"Blues\", ax=axes[(j,i)])\n",
    "    sns.kdeplot(X_minority[:,i], X_minority[:,j], alpha=0.5, cmap=\"Greens\", ax=axes[(j,i)])\n",
    "fig.savefig(f'../img/cat_sample_tr_iter_{trainer.G.training_iterations}.png',format='png', dpi=100)\n",
    "    #fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 90\n",
    "\n",
    "for _ in range(30):\n",
    "    trainer.train(data_loader, epochs)\n",
    "    \n",
    "    \n",
    "    if modus == 'full':\n",
    "        fake_minority = generator(*generator.sample_latent(num_samples= 1000, class_index=1)).data.numpy()\n",
    "        fake_majority = generator(*generator.sample_latent(num_samples= 1000, class_index=0)).data.numpy()\n",
    "    elif modus == 'minority':\n",
    "        fake_minority = generator(generator.sample_latent(num_samples= 1000)).data.numpy()\n",
    "        \n",
    "    fig, axes = plt.subplots(nrows=no_vars, ncols=no_vars, sharex=True, squeeze=True,figsize=(10,10))\n",
    "    for y in axes:\n",
    "        for x in y:\n",
    "            x.set_xticklabels([])\n",
    "            x.set_yticklabels([])\n",
    "    \n",
    "    for i in range(no_vars):\n",
    "        sns.kdeplot(X_minority[:,i], alpha=0.5, shade=True, color=\"blue\", ax=axes[(i,i)])\n",
    "        sns.kdeplot(fake_minority[:,i], alpha=0.5, shade=True, color=\"green\", ax=axes[(i,i)])\n",
    "    \n",
    "    for i,j in combinations:\n",
    "        axes[(i,j)].set_ylim(0,1)\n",
    "        # majority (upper right)\n",
    "        if modus == 'full':\n",
    "            sns.kdeplot(X_majority[0:1000,i], X_majority[0:1000,j], alpha=0.5, cmap=\"Blues\", ax=axes[(i,j)])\n",
    "            sns.kdeplot(fake_majority[:,i], fake_majority[:,j], alpha=0.5, cmap=\"Greens\", ax=axes[(i,j)], )\n",
    "        \n",
    "        # minority (lower left)\n",
    "        sns.kdeplot(X_minority[:,i], X_minority[:,j], alpha=0.5, cmap=\"Blues\", ax=axes[(j,i)])\n",
    "        sns.kdeplot(fake_minority[:,i], fake_minority[:,j], alpha=0.5, cmap=\"Greens\", ax=axes[(j,i)])\n",
    "        \n",
    "    fig.savefig(f'../img/cont_sample_tr_iter_{trainer.G.training_iterations}.png',format='png', dpi=200)\n",
    "        #fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fake_minority = generator(generator.sample_latent(num_samples= X_train_minority.shape[0], class_index=None))\n",
    "fake_minority = generator.sample_data(10000).numpy()\n",
    "minority = pd.DataFrame(X_train_minority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(minority.shape[1]):\n",
    "    plt.hist(X_train_minority[:,i], alpha=0.3, density=True)\n",
    "    plt.hist(fake_minority[:,i], alpha=0.3, density=True, color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.quantile(X_train_minority, q=np.arange(0,1,0.1), axis=0))\n",
    "print(np.quantile(fake_minority[0].data.numpy(), q=np.arange(0,1,0.1), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.cov(X_minority, rowvar=False) - np.cov(fake_minority,rowvar=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = X_train_minority.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fake = generator(*generator.sample_latent(num_samples= sample_size, class_index=1)).data.numpy()\n",
    "fake = generator.sample_data(num_samples= sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fakereal = np.vstack([X_train_minority, \n",
    "                        fake])\n",
    "y_fakereal = np.concatenate([np.zeros(X_train_minority.shape[0]), \n",
    "                        np.ones(fake.shape[0])]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=50, min_samples_leaf=10, n_jobs=10)\n",
    "model_fakereal = clf.fit(X_fakereal, y_fakereal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_fakereal = model_fakereal.predict_proba(X_fakereal)[:,1]\n",
    "print(accuracy_score(y_fakereal, pred_fakereal>0.5))\n",
    "roc_auc_score(y_fakereal, pred_fakereal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive performance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_auc(model_library, X, y_true):\n",
    "    auc = {}\n",
    "    for model in model_library.keys():\n",
    "        pred = model_library[model].predict_proba(X)[:,1]\n",
    "        auc[model] = roc_auc_score(y_true, pred)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minority_samples = X_minority.shape[0]\n",
    "majority_samples = X_majority.shape[0]\n",
    "\n",
    "fake_minority = generator(*generator.sample_latent(num_samples= minority_samples, class_index=1)).data.numpy()\n",
    "fake_majority = generator(*generator.sample_latent(num_samples= majority_samples, class_index=0)).data.numpy()\n",
    "\n",
    "X_synthetic = np.vstack([fake_majority, \n",
    "                         fake_minority])\n",
    "y_synthetic = np.concatenate([np.zeros(majority_samples), \n",
    "                              np.ones(minority_samples)]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_org = DecisionTreeClassifier(max_depth=10) #LogisticRegression(solver='saga') \n",
    "clf_fake = DecisionTreeClassifier(max_depth=10) #LogisticRegression(solver='saga')\n",
    "\n",
    "predictive = {}\n",
    "predictive[\"real\"] = clf_org.fit(X=X_train, y=y_train_bin)\n",
    "predictive[\"synthetic\"] = clf_fake.fit(X=X_synthetic, y=y_synthetic)\n",
    "\n",
    "test_auc(predictive, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampling_ratio = 4\n",
    "sample_size = int(X_train_minority.shape[0] * upsampling_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = {'train':{\"original\":[],\"GANbalanced\":[]}, 'test':{\"original\":[],\"GANbalanced\":[]}}\n",
    "for _ in range(10):\n",
    "    #X_fake = generator(*generator.sample_latent(num_samples= sample_size, class_index=1)).data.numpy()\n",
    "    X_fake = generator.sample_data(sample_size).numpy()\n",
    "    y_fake = np.ones(shape=[sample_size])\n",
    "\n",
    "    X_up = np.vstack([X_train,X_fake])\n",
    "    y_up = np.hstack([y_train_bin,y_fake])\n",
    "\n",
    "    clf_org = DecisionTreeClassifier(min_weight_fraction_leaf=0.01)\n",
    "    clf_fake = DecisionTreeClassifier(min_weight_fraction_leaf=0.01)\n",
    "\n",
    "    upsampling = {}\n",
    "    upsampling[\"original\"] =  clf_org.fit(X=X_train, y=y_train_bin)\n",
    "    upsampling[\"GANbalanced\"] = clf_fake.fit(X=X_up, y=y_up)\n",
    "    \n",
    "    performance_temp_train = test_auc(upsampling, X_train, y_train_bin)\n",
    "    performance_temp_test = test_auc(upsampling, X_test, y_test_bin)\n",
    "    \n",
    "    for model in performance_temp_test:\n",
    "        performance['train'][model].append(performance_temp_train[model])\n",
    "        performance['test'][model].append(performance_temp_test[model])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(performance['train']).mean())\n",
    "print(pd.DataFrame(performance['test']).mean())\n",
    "\n",
    "\n",
    "print(pd.DataFrame(performance['test']).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_function(X, y, clf, ax):\n",
    "    plot_step = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, alpha=0.4)\n",
    "    ax.scatter(X[:, 0], X[:, 1], alpha=0.8, c=y, edgecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
    "plot_decision_function(X_train, y_train, upsampling[\"original\"], ax1)\n",
    "plot_decision_function(X_up, y_up, upsampling[\"GANbalanced\"], ax2)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of dimensionality on SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With increasing dimensionality, we expect SMOTE's underlying nearest neighbor approach to fail to capture relevant neighborhoods. We measure SMOTE performance in terms of RF being able to differentiate between real and synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTENC, SMOTE\n",
    "from imblearn.under_sampling import TomekLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 320\n",
    "# Create single dataset to avoid random effects\n",
    "# Only works for all informative features\n",
    "X_full,y = make_classification(n_samples=10000, weights=[0.9,0.1], n_clusters_per_class=1,\n",
    "                              n_features=n_features, \n",
    "                              n_informative=n_features, \n",
    "                              n_redundant=0, n_repeated=0,\n",
    "                             random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y, \n",
    "                                                    stratify=y, test_size=0.5, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Oversampling test\n",
    "# Drop variables until desired dimensionality\n",
    "no_var_list = [5,10,20,40,80,160,320]\n",
    "auc_no_vars = []\n",
    "\n",
    "for k in no_var_list: #\n",
    "#     X_full,y = make_classification(n_samples=1000, weights=[0.9,0.1], n_clusters_per_class=1,\n",
    "#                               n_features=k, \n",
    "#                               n_informative=k, \n",
    "#                               n_redundant=0, n_repeated=0,\n",
    "#                              random_state=123)\n",
    "#     X_train_sample, X_test_sample, y_train, y_test = train_test_split(X_full, y, \n",
    "#                                                     stratify=y, test_size=0.5, random_state=123)\n",
    "    X_train_sample = X_train[:,0:k]\n",
    "    X_test_sample = X_test[:,0:k]\n",
    "    \n",
    "    # Sample synthetic SMOTE data\n",
    "    smote = SMOTE(sampling_strategy = {1:np.sum(y)*1}, k_neighbors=10,\n",
    "                  random_state=123, n_jobs=20)\n",
    "    X_smote, y_smote =  smote.fit_resample(X_train_sample,y_train)\n",
    "    \n",
    "    # Supplement original data\n",
    "    model_library = {\n",
    "        \"original\":RandomForestClassifier(n_estimators=100, min_samples_leaf=50, n_jobs=20),\n",
    "        \"smote\":RandomForestClassifier(n_estimators=100, min_samples_leaf=50, n_jobs=20)\n",
    "    }\n",
    "\n",
    "    model_library[\"original\"] = model_library['original'].fit(X=X_train_sample, y=y_train)\n",
    "    model_library[\"smote\"] = model_library['smote'].fit(X=X_smote, y=y_smote)\n",
    "    \n",
    "    temp = test_auc(model_library, X_test_sample, y_test)\n",
    "    auc_no_vars.append(temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_no_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_real_score(X_real, X_fake, random_state=None):\n",
    "    X_fakereal = np.vstack([X_real, X_fake])\n",
    "    y_fakereal = np.concatenate([np.zeros(X_real.shape[0]), \n",
    "                                 np.ones( X_fake.shape[0])]).flatten()\n",
    "    \n",
    "    X_fakereal_train, X_fakereal_test, y_fakereal_train, y_fakereal_test =\\\n",
    "        train_test_split(X_fakereal, y_fakereal, test_size=0.5, random_state=random_state)\n",
    "    clf = RandomForestClassifier(n_estimators=100, min_weight_fraction_leaf=0.05, n_jobs=20, random_state=random_state)\n",
    "    model_fakereal = clf.fit(X_fakereal_train, y_fakereal_train)\n",
    "\n",
    "    pred_fakereal = model_fakereal.predict_proba(X_fakereal_test)[:,1]\n",
    "    return roc_auc_score(y_fakereal_test, pred_fakereal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Discriminator test\n",
    "# Drop variables until desired dimensionality\n",
    "auc = {}\n",
    "\n",
    "for k in [5,10,20,40,80,160,320]: #\n",
    "    X = X_full[:,0:k]\n",
    "    # Sample synthetic SMOTE data\n",
    "    smote = SMOTE(sampling_strategy = {1:np.sum(y)*2}, k_neighbors=5,\n",
    "                  random_state=123, n_jobs=20)\n",
    "    X_smote, y_smote = smote.fit_sample(X,y)\n",
    "   \n",
    "    auc[k] = fake_real_score(X[y==1],X_smote, random_state=123)\n",
    "    \n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Discriminator test\n",
    "# Drop variables until desired dimensionality\n",
    "auc = {}\n",
    "\n",
    "for k in [5,10,20,40,80,160,320]: #\n",
    "    X = X_full[:,0:k]\n",
    "    # Sample synthetic GAN data\n",
    "    # TODO: Train GAN generator\n",
    "    X_gan = generator.sample_data(np.sum(y)*2)\n",
    "   \n",
    "    auc[k] = fake_real_score(X_gan,X_smote, random_state=123)\n",
    "    \n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(auc.keys(), auc.values())\n",
    "plt.xlabel(\"No. of variables (10,000 minority observations )\")\n",
    "plt.ylabel(\"Discriminator AUC (SMOTE)\")\n",
    "plt.savefig(\"../img/SMOTE_performance_over_variables_10k_minority.png\", format='png',dpi=200)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [deeplearning]",
   "language": "python",
   "name": "Python [deeplearning]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
