{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TableGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a summary of changes introduced by Wasserstein GANs, see the original paper or https://www.alexirpan.com/2017/02/22/wasserstein-gan.html:\n",
    "\n",
    "- Discriminator gives continuous output\n",
    "- Loss is the difference in mean output (Critic) and mean output on generated data for optimal critic (generator)\n",
    "- Apply gradient regularization to ensure the assumption that the critic is a K-Lipschitz function\n",
    "\n",
    "Thanks to the pytorch implementation in https://github.com/caogang/wgan-gp/blob/master/gan_toy.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "# Variable provides a wrapper around tensors to allow automatic differentiation, etc.\n",
    "from torch.autograd.variable import Variable \n",
    "import torch.autograd as autograd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimulationDataset(Dataset):\n",
    "    \"\"\"Simulated dataset with continuous and categorical variables.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "        \"\"\"\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #observation = self.data_frame.iloc[idx, 1:].values\n",
    "        observation = self.data_frame.iloc[idx, :].values\n",
    "        label = self.data_frame.loc[idx,\"group\"]\n",
    "        return (observation, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_noise(size):\n",
    "    \"\"\"\n",
    "    Generates a vector with length 100 of Gaussian noise with (batch_size, 100)\n",
    "    \"\"\"\n",
    "    n = Variable(\n",
    "        torch.randn(size, 100) # random values from standard normal\n",
    "    )\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A three-layer generative neural network\n",
    "    Assumes that the input data is sorted continuous, then categorical\n",
    "    output_continuous: Number of continuous variables\n",
    "    output_continuous: Number of binary variables\n",
    "    output_categorical: List of number of columns each categorical variable\n",
    "    \"\"\"\n",
    "    def __init__(self, n_output_continuous, n_output_binary, n_output_categorical, noise_dim=100):\n",
    "        super().__init__()\n",
    "        self.n_output_continuous = n_output_continuous\n",
    "        self.n_output_binary = n_output_binary\n",
    "        self.n_output_categorical = n_output_categorical\n",
    "        \n",
    "        self.hidden0 = nn.Sequential(\n",
    "            nn.Linear(noise_dim, 128),\n",
    "            nn.LeakyReLU(0.2)\n",
    "            # TODO: Why no dropout in generator?\n",
    "        )\n",
    "        \n",
    "        self.hidden1 = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        self.hidden2 = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        self.binary = [nn.Sequential(\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        ) \n",
    "                            for x in range(n_output_binary)]\n",
    "        \n",
    "        self.categorical = [nn.Sequential(\n",
    "            nn.Linear(256, x),\n",
    "            nn.Softmax(dim=0)\n",
    "        ) \n",
    "                            for x in n_output_categorical]\n",
    "        \n",
    "        self.continuous = nn.Sequential(\n",
    "            nn.Linear(256, n_output_continuous),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.hidden0(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.hidden2(x)\n",
    "        out_binary = [self.binary[var](x) for var in \n",
    "                   range(self.n_output_binary)]    \n",
    "        out_categorical = [self.categorical[var](x)\n",
    "                           for var in \n",
    "                           range(len(self.n_output_categorical))]    \n",
    "        out_continuous = self.continuous(x)\n",
    "        return torch.cat((out_continuous,*out_binary, *out_categorical), dim=1)\n",
    "    \n",
    "    def sample(self, x):\n",
    "        x = self.hidden0(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.hidden2(x)\n",
    "        out_binary = [(self.binary[var](x)>0.5).float() for var in \n",
    "                   range(self.n_output_binary)]    \n",
    "        out_categorical = [torch.eye(self.n_output_categorical[var])[torch.multinomial(self.categorical[var](x),1).squeeze()]\n",
    "                            #[torch.eye(self.n_output_categorical[var])[torch.argmax(self.categorical[var](x), dim=1)] \n",
    "                           for var in \n",
    "                           range(len(self.n_output_categorical))]\n",
    "        \n",
    "        out_continuous = self.continuous(x)\n",
    "        return torch.cat((out_continuous,*out_binary, *out_categorical), dim=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A three hidden-layer discriminative neural network\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__() # get the __init__() from the parent module\n",
    "        input_dim\n",
    "        n_out = 1\n",
    "        \n",
    "        self.hidden0 = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256), # Linear transformation part input*W+b\n",
    "            nn.LeakyReLU(0.2), # leaky relu is more robust for GANs than ReLU\n",
    "            nn.Dropout(0.2)\n",
    "            )\n",
    "        \n",
    "        self.hidden1 = nn.Sequential(\n",
    "            nn.Linear(256, 256), # Linear transformation part input*W+b\n",
    "            nn.LeakyReLU(0.2), # leaky relu is more robust for GANs than ReLU\n",
    "            nn.Dropout(0.2)\n",
    "            )\n",
    "            \n",
    "        self.hidden2 = nn.Sequential(\n",
    "            nn.Linear(256, 128), # Linear transformation part input*W+b\n",
    "            nn.LeakyReLU(0.2), # leaky relu is more robust for GANs than ReLU\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(128, n_out), # Linear transformation part input*W+b\n",
    "        )\n",
    "    \n",
    "    # Careful to make forward() a function of the net, not of __init__\n",
    "    def forward(self, x):\n",
    "        x = self.hidden0(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_data(real_data, fake_data):\n",
    "    eps = torch.rand(real_data.size(0), 1) # A random unif number for each obs in the batch\n",
    "    eps = eps.expand(real_data.size()) # Can only multiply tensors with tensors, so expand to same dimensions\n",
    "    interpolated_data = eps*real_data + (1-eps)*fake_data\n",
    "    interpolated_data = Variable(interpolated_data, requires_grad=True) # Transform into Variable again\n",
    "    return interpolated_data\n",
    "\n",
    "def calc_gradient_penalty(critic, real_data, fake_data):\n",
    "    interpolated_data = interpolate_data(real_data, fake_data)\n",
    "    critic_output = critic(interpolated_data)\n",
    "    gradients = autograd.grad(inputs=interpolated_data, outputs=critic_output,\n",
    "                             grad_outputs=torch.ones(critic_output.size()),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    gradient_penalty=((gradients.norm(2, dim=1)-1) ** 2)\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def critic_loss_GP(critic, real_data, fake_data, penalty_coefficient):\n",
    "#     \"\"\"\n",
    "#     Wasserstein distance to minimize as loss for the critic, regularized by\n",
    "#     Lipschwitz 1 gradient penalty\n",
    "#     -(E[D(x_real)] - E[D(x_fake)]) + lambda*E[(||D(x_imputed)'||_2 -1)**2]\n",
    "#     \"\"\"\n",
    "#     # Original critic loss\n",
    "#     # D(x_real)\n",
    "#     output_real = critic.forward(real_data)\n",
    "#     # D(x_fake)\n",
    "#     output_fake = critic.forward(fake_data)\n",
    "#     raw_loss = (output_fake - output_real).squeeze()\n",
    "    \n",
    "#     # Gradient penalty for Lipschwitz-1\n",
    "#     gradient_penalty = calc_gradient_penalty(critic, real_data,fake_data)\n",
    "    \n",
    "#     # Total loss\n",
    "#     loss = (raw_loss + penalty_coefficient * gradient_penalty).mean()\n",
    "#     return loss\n",
    "\n",
    "def critic_loss(output_real, output_fake):\n",
    "    \"\"\"\n",
    "    Wasserstein distance to minimize as loss for the critic\n",
    "    -(E[D(x_real)] - E[D(x_fake)]) \n",
    "    \"\"\"\n",
    "    return -( torch.mean(output_real) - torch.mean(output_fake) )\n",
    "\n",
    "def generator_loss(output_fake):\n",
    "    \"\"\"\n",
    "    Loss to minimize for the generator on the output of the optimal critic\n",
    "    -E[D(G(noise))]\n",
    "    \"\"\"\n",
    "    return -torch.mean(output_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_critic(optimizer, real_data, fake_data, gradient_penalty_coefficient=10):\n",
    "    N = real_data.size(0) # Get number of rows from torch tensor\n",
    "    optimizer.zero_grad() # reset gradient\n",
    "\n",
    "    # Note: Calling backward() multiple times will acumulate the gradients\n",
    "    # until they are reset with zero_grad()\n",
    "    # E[D(x_real)]\n",
    "    output_real = critic.forward(real_data)\n",
    "\n",
    "    # E[D(x_fake)]\n",
    "    output_fake = critic.forward(fake_data)\n",
    "    raw_loss = critic_loss(output_real, output_fake)\n",
    "\n",
    "    # Gradient penalty\n",
    "    gradient_penalty = calc_gradient_penalty(critic, real_data,fake_data)\n",
    "\n",
    "    # Calculate overall loss\n",
    "    # Minimize the raw loss pushed upwards by penalty (always positive)\n",
    "    loss = raw_loss + gradient_penalty_coefficient*gradient_penalty\n",
    "    loss = loss.mean() # Average over batch\n",
    "\n",
    "    # Weight update\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Return error and predictions for monitoring\n",
    "    return raw_loss.mean(), output_real, output_fake\n",
    "\n",
    "def train_generator(optimizer, fake_data):\n",
    "    N = fake_data.size(0) # Get number of rows from torch tensor\n",
    "    optimizer.zero_grad() # reset gradient\n",
    "\n",
    "    # Get discriminator prediction output\n",
    "    critic_prediction = critic.forward(fake_data)\n",
    "\n",
    "    # See explanation above. Intuitively, we create loss if the \n",
    "    # discriminator predicts our pseudo-ones as zeros.\n",
    "    loss_generator = generator_loss(critic_prediction)\n",
    "    loss_generator.backward()\n",
    "\n",
    "    # Weight update\n",
    "    optimizer.step()\n",
    "\n",
    "    # Return error and predictions for monitoring\n",
    "    return loss_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = SimulationDataset(\"../simulation_data/simulation.csv\")\n",
    "val_data = torch.from_numpy(pd.read_csv(\"../simulation_data/simulation_val.csv\").values).float()\n",
    "validation_noise = make_noise(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_performance = []\n",
    "generator_performance = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GeneratorNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ed947c0aceca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGeneratorNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_output_continuous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_output_binary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_output_categorical\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcritic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCriticNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GeneratorNet' is not defined"
     ]
    }
   ],
   "source": [
    "generator = GeneratorNet(n_output_continuous=7,n_output_binary=2,n_output_categorical=[3])\n",
    "critic = CriticNet(input_dim=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "learning_rate = 1e-4\n",
    "critic_rounds = 5\n",
    "gradient_penalty_coefficient = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_optimizer = optim.Adam(critic.parameters(), lr=learning_rate, )\n",
    "generator_optimizer = optim.Adam(generator.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # enumerate() outputs index, value for an indexable object\n",
    "    # output the index of the batch and the output of data_loader\n",
    "    # data_loader() outputs a batch of images and their label (which we don't need in this case)\n",
    "    for n_batch, (real_batch,_) in enumerate(data_loader):\n",
    "        N = real_batch.size(0) # Get the number of images from tensor\n",
    "\n",
    "        ## Train discriminator\n",
    "        # Collect real data\n",
    "        real_data = Variable(real_batch.float())\n",
    "\n",
    "        temp_performance = []\n",
    "        for k in range(critic_rounds):\n",
    "            # Create fake data\n",
    "            fake_data = generator(make_noise(N)).detach() \n",
    "            # generator() creates a graph on the fly, which we drop after collecting the fake data\n",
    "            disc_error, disc_pred_real, disc_pred_fake = train_critic(real_data = real_data, fake_data = fake_data, \n",
    "                                                                      optimizer = critic_optimizer,\n",
    "                                                                      gradient_penalty_coefficient=gradient_penalty_coefficient)\n",
    "            #temp_performance.append(disc_error.detach().cpu().numpy())\n",
    "        #critic_performance.append(-np.mean(temp_performance))\n",
    "        critic_performance.append(-disc_error.detach().cpu().numpy())\n",
    "\n",
    "        ## Train generator\n",
    "        fake_data = generator(make_noise(N))\n",
    "        # This time we keep the graph, because we backprop on it in the training function\n",
    "        gen_error = train_generator(optimizer = generator_optimizer, fake_data = fake_data)\n",
    "        generator_performance.append(gen_error.detach().cpu().numpy())\n",
    "\n",
    "        if n_batch % 50 ==0 and val_data is not None:\n",
    "            print(\"{train:.6f} | {val:.6f}\".format(\n",
    "                train=(critic(real_data).mean() - critic(generator(make_noise(len(val_data)))).mean()).detach().cpu().numpy(),\n",
    "                val  =(critic(val_data).mean()  - critic(generator(make_noise(len(val_data)))).mean()).detach().cpu().numpy() \n",
    "            ))\n",
    "            #print(pd.DataFrame(generator(validation_noise).detach().numpy()).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tableGAN.simulation import create_GAN_data\n",
    "from tableGAN.tableGAN import make_noise\n",
    "from tableGAN import tableGAN\n",
    "\n",
    "generator = tableGAN.GeneratorNet(n_output_continuous=7,n_output_binary=2,n_output_categorical=[3])\n",
    "critic = tableGAN.CriticNet(input_dim=12)\n",
    "wgan = tableGAN.WGAN(generator, critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.319896 | 0.305091\n",
      "17.684021 | 16.547075\n",
      "5.628733 | 6.530212\n",
      "4.506333 | 4.725708\n",
      "5.565834 | 5.130300\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-18ac84f8a6ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_penalty_coefficient\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mgradient_penalty_coefficient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcritic_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcritic_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     val_data=val_data)\n\u001b[0m",
      "\u001b[0;32m~/Seafile/deeplearning_marketing/code/tableGAN/tableGAN.py\u001b[0m in \u001b[0;36mtrain_WGAN\u001b[0;34m(self, data_loader, critic_optimizer, generator_optimizer, num_epochs, critic_rounds, gradient_penalty_coefficient, val_data)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritic_rounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                     \u001b[0;31m# Create fake data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                     \u001b[0mfake_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m                     \u001b[0;31m# generator() creates a graph on the fly, which we drop after collecting the fake data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                     disc_error, disc_pred_real, disc_pred_fake = self.train_critic(real_data = real_data, fake_data = fake_data,\n",
      "\u001b[0;32m~/anaconda/envs/deeplearning/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Seafile/deeplearning_marketing/code/tableGAN/tableGAN.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         out_binary = [self.binary[var](x) for var in\n\u001b[0;32m---> 72\u001b[0;31m                    range(self.n_output_binary)]\n\u001b[0m\u001b[1;32m     73\u001b[0m         out_categorical = [self.categorical[var](x)\n\u001b[1;32m     74\u001b[0m                            \u001b[0;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Seafile/deeplearning_marketing/code/tableGAN/tableGAN.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         out_binary = [self.binary[var](x) for var in\n\u001b[0m\u001b[1;32m     72\u001b[0m                    range(self.n_output_binary)]\n\u001b[1;32m     73\u001b[0m         out_categorical = [self.categorical[var](x)\n",
      "\u001b[0;32m~/anaconda/envs/deeplearning/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/deeplearning/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/deeplearning/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "critic_performance, generator_performance = wgan.train_WGAN(\n",
    "    data_loader=data_loader, critic_optimizer=critic_optimizer, generator_optimizer=generator_optimizer,\n",
    "    num_epochs =num_epochs, gradient_penalty_coefficient= gradient_penalty_coefficient,\n",
    "    critic_rounds=critic_rounds,\n",
    "    val_data=val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(critic_performance[10:], color = \"red\")\n",
    "plt.plot(generator_performance[10:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = pd.read_csv(\"../simulation_data/simulation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = pd.DataFrame(generator.sample(make_noise(20000)).detach().numpy())\n",
    "fake.iloc[:,0] = fake.iloc[:,0].round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([np.round(np.mean(real, axis=0),4), np.round(np.mean(fake, axis=0),4)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(fake.shape[1]):\n",
    "    plt.hist(real.iloc[:,i], alpha=0.5, bins=25, density=True)\n",
    "    plt.hist(fake.iloc[:,i], alpha=0.5, bins=25, density=True, color=\"orange\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(real.iloc[:,[7,8]].groupby(\"group\").apply(np.mean, axis=0))\n",
    "print(fake.iloc[:,[7,8]].groupby(7).apply(np.mean, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(real.iloc[:,[7,9,10,11]].groupby(\"group\").apply(np.mean, axis=0))\n",
    "print(fake.iloc[:,[7,9,10,11]].groupby(7).apply(np.mean, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(np.cov(real.iloc[:,4:7].values, rowvar=False), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cov(fake.iloc[:,4:7].values, rowvar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [deeplearning]",
   "language": "python",
   "name": "Python [deeplearning]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
