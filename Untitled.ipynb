{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD, Adam\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tableGAN\n",
    "from tableGAN.utils import create_GAN_data, TabularDataset, Preprocessor\n",
    "from tableGAN.tableGAN import make_noise, CriticNet, WGAN, GeneratorNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = create_GAN_data(20000,class_ratio=0.05,random_state=123)\n",
    "test = create_GAN_data(20000,class_ratio=0.05,random_state=123)\n",
    "minority = training.groupby(\"group\").get_group(1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=training.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dims = [(1,5),(1,5),(3,10)]\n",
    "generator = GeneratorNet(noise_dim=10, lin_layer_sizes=[10,10,10], lin_layer_dropouts=None,\n",
    "                        no_of_cont=7, emb_dims=emb_dims)\n",
    "#generator = GeneratorNet(noise_dim=100,n_output_continuous=7,n_output_binary=2,n_output_categorical=[3])\n",
    "critic = CriticNet(no_of_cont=7, emb_dims=emb_dims, lin_layer_sizes=[10,10,10], \n",
    "                   emb_dropout = 0, lin_layer_dropouts = [0,0,0])\n",
    "wgan = WGAN(generator, critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "#learning_rate = 1e-5\n",
    "critic_rounds = 5\n",
    "gradient_penalty_coefficient = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "minority_tab = TabularDataset(minority, cat_cols=['group',8,9,10,11])\n",
    "data_loader = DataLoader(minority_tab, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_optimizer = Adam(critic.parameters(), lr=1e-3)\n",
    "generator_optimizer = Adam(generator.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.218724 | \n",
      "3.978997 | \n",
      "5.971148 | \n",
      "8.126633 | \n",
      "10.938429 | \n",
      "10.490806 | \n",
      "13.776539 | \n",
      "19.359386 | \n",
      "35.364006 | \n",
      "46.441959 | \n",
      "95.632416 | \n",
      "118.584999 | \n",
      "229.817413 | \n",
      "495.755554 | \n",
      "957.134644 | \n",
      "1626.579468 | \n",
      "2495.883545 | \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b566e930ee37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_penalty_coefficient\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mgradient_penalty_coefficient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcritic_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcritic_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     val_data=None) #torch.from_numpy(test.values).float()\n\u001b[0m",
      "\u001b[0;32m~/Seafile/deeplearning_marketing/GANbalanced/tableGAN/tableGAN.py\u001b[0m in \u001b[0;36mtrain_WGAN\u001b[0;34m(self, data_loader, critic_optimizer, generator_optimizer, num_epochs, critic_rounds, gradient_penalty_coefficient, val_data)\u001b[0m\n\u001b[1;32m    459\u001b[0m                     disc_error, disc_pred_real, disc_pred_fake = self.train_critic(real_data = real_data, fake_data = fake_data,\n\u001b[1;32m    460\u001b[0m                                                                               \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m                                                                               gradient_penalty_coefficient=gradient_penalty_coefficient)\n\u001b[0m\u001b[1;32m    462\u001b[0m                     \u001b[0;31m#temp_performance.append(disc_error.detach().cpu().numpy())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m                 \u001b[0;31m#critic_performance.append(-np.mean(temp_performance))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Seafile/deeplearning_marketing/GANbalanced/tableGAN/tableGAN.py\u001b[0m in \u001b[0;36mtrain_critic\u001b[0;34m(self, optimizer, real_data, fake_data, gradient_penalty_coefficient)\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;31m# a joint data array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         gradient_penalty = self.calc_gradient_penalty(real_data,\n\u001b[0;32m--> 512\u001b[0;31m                                                  fake_data)\n\u001b[0m\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;31m# Calculate overall loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Seafile/deeplearning_marketing/GANbalanced/tableGAN/tableGAN.py\u001b[0m in \u001b[0;36mcalc_gradient_penalty\u001b[0;34m(self, real_data, fake_data)\u001b[0m\n\u001b[1;32m    554\u001b[0m         gradients = autograd.grad(inputs=[interpolated_data[0], interpolated_data[1]], outputs=critic_output,\n\u001b[1;32m    555\u001b[0m                                  \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritic_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m                                   create_graph=True, retain_graph=True)[0] #, only_inputs=True\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/deeplearning/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    143\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    144\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         inputs, allow_unused)\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "critic_performance, generator_performance = wgan.train_WGAN(\n",
    "    data_loader=data_loader, critic_optimizer=critic_optimizer, generator_optimizer=generator_optimizer,\n",
    "    num_epochs =num_epochs, gradient_penalty_coefficient= gradient_penalty_coefficient,\n",
    "    critic_rounds=critic_rounds,\n",
    "    val_data=None) #torch.from_numpy(test.values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = pd.DataFrame(wgan.generator.sample(make_noise(20000, dim=wgan.generator.noise_dim)).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.vstack([np.round(np.mean(minority, axis=0),4), \n",
    "                        np.round(np.mean(fake.values, axis=0),4)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [deeplearning]",
   "language": "python",
   "name": "Python [deeplearning]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
