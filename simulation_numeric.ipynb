{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as sp\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import minmax_scale, scale, MinMaxScaler\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wgan.models import Generator, Discriminator\n",
    "from wgan.training import Trainer\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, brier_score_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from imbalanced_sampler.sampler import ImbalancedDatasetSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_correlation_matrix(no_var):\n",
    "    corr = np.zeros([no_var,no_var])\n",
    "    corr_temp = np.random.uniform(-1,1,size=[(no_var-1)*2])\n",
    "    corr[np.triu_indices(no_var, 1)] = corr_temp\n",
    "    corr + corr.T + np.eye(no_var)\n",
    "    return corr\n",
    "\n",
    "\n",
    "def create_continuous_data(N, pos_ratio=0, noise_ratio=0, no_var=10, cov=None, random_state=None):\n",
    "    if random_state is not None: np.random.seed(random_state)\n",
    "    # Group indicator\n",
    "    #group = sp.binom.rvs(p=0.25, n=1, size=N    \n",
    "    N_neg = int(N*(1-pos_ratio))\n",
    "    N_pos = N-N_neg\n",
    "    y = np.concatenate([np.zeros(N_neg), np.ones(N_pos)])\n",
    "    \n",
    "    mean = np.random.uniform(size=no_var)\n",
    "    mean0 = np.random.normal(loc=mean,scale=0.5)\n",
    "    mean1 = np.random.normal(loc=mean,scale=0.5)\n",
    "    \n",
    "    if cov is None: \n",
    "        cov0 = sp.invwishart.rvs(df=no_var*2, scale=np.eye(no_var))\n",
    "        cov1 = sp.invwishart.rvs(df=no_var*2, scale=np.eye(no_var))\n",
    "\n",
    "    # Noise are variables with same distribution in majority and minority class\n",
    "    if noise_ratio != 0:  \n",
    "        no_noise = int(noise_ratio*no_var)\n",
    "        no_var = no_var - no_noise\n",
    "        X_noise = sp.multivariate_normal.rvs(mean=mean0[no_var:], cov=cov0[no_var:,no_var:], size=N).reshape([N,-1])\n",
    "\n",
    "    X1 = sp.multivariate_normal.rvs(mean=mean1[0:no_var], cov= cov1[:no_var,:no_var], size=N_pos)\n",
    "    X0 = sp.multivariate_normal.rvs(mean=mean0[0:no_var], cov= cov0[:no_var,:no_var], size=N_neg)\n",
    "    X = np.vstack([X0,X1])\n",
    "    X = np.hstack([X, X_noise])\n",
    "    \n",
    "    return {\"X\":X, \"y\":y,\"mean0\":mean0,\"mean1\":mean1, \"cov0\":cov0, \"cov1\":cov1}\n",
    "\n",
    "def create_dataset(n_samples=1000, n_features=2, n_classes=3, weights=(0.01, 0.01, 0.98),\n",
    "                   class_sep=0.8, n_clusters=1, random_state=0):\n",
    "    return make_classification(n_samples=n_samples,\n",
    "                               n_informative=2, n_redundant=0, n_repeated=0,\n",
    "                               n_classes=n_classes, n_features = n_features,\n",
    "                               n_clusters_per_class=n_clusters,\n",
    "                               weights=list(weights),\n",
    "                               class_sep=class_sep, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artifical Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Characterizes a Dataset for PyTorch\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        data: pandas data frame\n",
    "          The data frame object for the input data. It must\n",
    "          contain all the continuous, categorical and the\n",
    "          output columns to be used.\n",
    "\n",
    "        cat_cols: List of strings\n",
    "          The names of the categorical columns in the data.\n",
    "          These columns will be passed through the embedding\n",
    "          layers in the model. These columns must be\n",
    "          label encoded beforehand.\n",
    "\n",
    "        output_col: string\n",
    "          The name of the output variable column in the data\n",
    "          provided.\n",
    "        \"\"\"\n",
    "\n",
    "        self.n = X.shape[0]\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Denotes the total number of samples.\n",
    "        \"\"\"\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Generates one sample of data.\n",
    "        \"\"\"\n",
    "        return [self.X[idx], self.y[idx],]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "modus = 'full' #'full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_vars = 4\n",
    "N= 10000\n",
    "\n",
    "data = create_continuous_data(N, pos_ratio=0.1, noise_ratio=0.5, no_var=no_vars, random_state=123) #, cov=np.eye(no_vars)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['X'], data['y'], \n",
    "                                                    stratify=data['y'], test_size=0.5, random_state=123)\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train_majority = X_train[y_train==0,:]\n",
    "X_train_minority = X_train[y_train==1,:]\n",
    "\n",
    "y_train_bin = y_train[:]\n",
    "y_temp = np.zeros([len(y_train),2])\n",
    "y_temp[y_train==0,0] = 1\n",
    "y_temp[y_train==1,1] = 1\n",
    "y_train = y_temp\n",
    "\n",
    "#mean_minority = np.mean(X_minority, axis=0)\n",
    "#sd_minority = np.std(X_minority, axis=0)\n",
    "#X_minority = (X_minority-mean_minority)/sd_minority\n",
    "\n",
    "if modus == 'minority':\n",
    "    dataset = TabularDataset(X_train_minority, y_train[np.argmax(y_train, axis=1),:])\n",
    "elif modus == 'full':\n",
    "    dataset = TabularDataset(X_train, y_train)\n",
    "else:\n",
    "    stop(\"Check modus. Must be one of ['minority, 'full]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49038391 0.30634059 0.49992896 0.47579336]\n",
      "[0.16135217 0.11213928 0.1387724  0.14682545]\n",
      "[0.34243136 0.59107512 0.49871203 0.48892863]\n",
      "[0.05684238 0.11284451 0.13446538 0.1420298 ]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(X_train_minority, axis=0))\n",
    "print(np.std(X_train_minority, axis=0))\n",
    "print(np.mean(X_train_majority, axis=0))\n",
    "print(np.std(X_train_majority, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_vars = 12\n",
    "# X, y = create_dataset(n_samples=200000, n_classes=2, weights=(0.05,0.95), n_features=no_vars,\n",
    "#                      n_clusters=1, class_sep=0.8, random_state=123)\n",
    "\n",
    "# X = minmax_scale(X)\n",
    "# X_majority = X[y==0,:]\n",
    "# X_minority = X[y==1,:]\n",
    "\n",
    "# dataset = TabularDataset(X_minority, y[y==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampler = ImbalancedDatasetSampler(labels = y_train_bin, num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "data_loader = DataLoader(dataset, batch_size = batch_size, shuffle=True)\n",
    "\n",
    "# Balanced sampling through inverse propensiImbalancedDatasetSampler(labels = list(y_train), num_samples=batch_size)ty\n",
    "#data_loader = DataLoader(dataset, batch_size = batch_size, \n",
    "#                     sampler = sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (lin_layers): ModuleList(\n",
      "    (0): Linear(in_features=26, out_features=128, bias=True)\n",
      "    (1): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=256, out_features=4, bias=True)\n",
      ")\n",
      "Discriminator(\n",
      "  (lin_layers): ModuleList(\n",
      "    (0): Linear(in_features=6, out_features=256, bias=True)\n",
      "    (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "generator = Generator(latent_dim=24, lin_layer_sizes=[128,256,256], output_dim=no_vars, aux_dim=2)\n",
    "\n",
    "discriminator = Discriminator(input_size=no_vars, lin_layer_sizes=[256,256], aux_input_size=2)\n",
    "\n",
    "print(generator)\n",
    "print(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizers\n",
    "lr_G = 5e-5\n",
    "lr_D = 5e-5\n",
    "betas = (.9, .99)\n",
    "G_optimizer = optim.Adam(generator.parameters(), lr=lr_G, betas=betas)\n",
    "D_optimizer = optim.Adam(discriminator.parameters(), lr=lr_D, betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(generator, discriminator, G_optimizer, D_optimizer, print_every=1000,\n",
    "                  use_cuda=torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.gp_weight = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "#epochs = 100\n",
    "#trainer.train(data_loader, epochs,  save_training_gif=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generator(generator.sample_latent(num_samples= 1000)).data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = [(x,y) for x in range(no_vars) for y in range(no_vars) if y>x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=no_vars, ncols=no_vars, sharex=True, sharey=True, squeeze=True,figsize=(10,10))\n",
    "for y in axes:\n",
    "    for x in y:\n",
    "        x.set_xticklabels([])\n",
    "        x.set_yticklabels([])\n",
    "\n",
    "for i,j in combinations:\n",
    "    sns.kdeplot(X_majority[:,i], X_majority[:,j], alpha=0.5, cmap=\"Blues\", ax=axes[(j,i)])\n",
    "    sns.kdeplot(X_minority[:,i], X_minority[:,j], alpha=0.5, cmap=\"Greens\", ax=axes[(j,i)])\n",
    "fig.savefig(f'../img/cont_sample_tr_iter_{trainer.G.training_iterations}.png',format='png', dpi=100)\n",
    "    #fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 90\n",
    "\n",
    "for _ in range(30):\n",
    "    trainer.train(data_loader, epochs)\n",
    "    \n",
    "    \n",
    "    if modus == 'full':\n",
    "        fake_minority = generator(*generator.sample_latent(num_samples= 1000, class_index=1)).data.numpy()\n",
    "        fake_majority = generator(*generator.sample_latent(num_samples= 1000, class_index=0)).data.numpy()\n",
    "    elif modus == 'minority':\n",
    "        fake_minority = generator(generator.sample_latent(num_samples= 1000)).data.numpy()\n",
    "        \n",
    "    fig, axes = plt.subplots(nrows=no_vars, ncols=no_vars, sharex=True, squeeze=True,figsize=(10,10))\n",
    "    for y in axes:\n",
    "        for x in y:\n",
    "            x.set_xticklabels([])\n",
    "            x.set_yticklabels([])\n",
    "    \n",
    "    for i in range(no_vars):\n",
    "        sns.kdeplot(X_minority[:,i], alpha=0.5, shade=True, color=\"blue\", ax=axes[(i,i)])\n",
    "        sns.kdeplot(fake_minority[:,i], alpha=0.5, shade=True, color=\"green\", ax=axes[(i,i)])\n",
    "    \n",
    "    for i,j in combinations:\n",
    "        axes[(i,j)].set_ylim(0,1)\n",
    "        # majority (upper right)\n",
    "        if modus == 'full':\n",
    "            sns.kdeplot(X_majority[0:1000,i], X_majority[0:1000,j], alpha=0.5, cmap=\"Blues\", ax=axes[(i,j)])\n",
    "            sns.kdeplot(fake_majority[:,i], fake_majority[:,j], alpha=0.5, cmap=\"Greens\", ax=axes[(i,j)], )\n",
    "        \n",
    "        # minority (lower left)\n",
    "        sns.kdeplot(X_minority[:,i], X_minority[:,j], alpha=0.5, cmap=\"Blues\", ax=axes[(j,i)])\n",
    "        sns.kdeplot(fake_minority[:,i], fake_minority[:,j], alpha=0.5, cmap=\"Greens\", ax=axes[(j,i)])\n",
    "        \n",
    "    fig.savefig(f'../img/cont_sample_tr_iter_{trainer.G.training_iterations}.png',format='png', dpi=200)\n",
    "        #fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = f\"multinormal_n{N//1000}_k{no_vars}_{modus}\"\n",
    "torch.save(generator.state_dict(), f\"../models/wgan_generator_{desc}_{generator.training_iterations}\")\n",
    "torch.save(discriminator.state_dict(), f\"../models/wgan_discriminator_{desc}_{generator.training_iterations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"multinormal_n10_k4_c2_6999\"\n",
    "generator.load_state_dict(torch.load(f\"../models/wgan_generator_{file_name}\"))\n",
    "discriminator.load_state_dict(torch.load(f\"../models/wgan_discriminator_{file_name}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_minority = generator(*generator.sample_latent(num_samples= minority_samples, class_index=1)).data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(X_minority, axis=0))\n",
    "print(np.mean(fake_minority, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.quantile(X_minority, q=np.arange(0,1,0.1), axis=0))\n",
    "print(np.quantile(fake_minority, q=np.arange(0,1,0.1), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.cov(X_minority, rowvar=False) - np.cov(fake_minority,rowvar=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = X_minority.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = generator(*generator.sample_latent(num_samples= sample_size, class_index=1)).data.numpy()\n",
    "#fake = generator(generator.sample_latent(num_samples= sample_size)).data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fakereal = np.vstack([X_minority, \n",
    "                        fake])\n",
    "y_fakereal = np.concatenate([np.zeros(X_minority.shape[0]), \n",
    "                        np.ones(fake.shape[0])]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=50, min_samples_leaf=20, n_jobs=10)\n",
    "model_fakereal = clf.fit(X_fakereal, y_fakereal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_fakereal = model_fakereal.predict_proba(X_fakereal)[:,1]\n",
    "roc_auc_score(y_fakereal, pred_fakereal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive performance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_bin = np.argmax(y_train, axis=1)\n",
    "y_test_bin = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_auc(model_library, X, y_true):\n",
    "    auc = {}\n",
    "    for model in model_library.keys():\n",
    "        pred = model_library[model].predict_proba(X_test)[:,1]\n",
    "        auc[model] = roc_auc_score(y_true, pred)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minority_samples = X_minority.shape[0]\n",
    "majority_samples = X_majority.shape[0]\n",
    "\n",
    "fake_minority = generator(*generator.sample_latent(num_samples= minority_samples, class_index=1)).data.numpy()\n",
    "fake_majority = generator(*generator.sample_latent(num_samples= majority_samples, class_index=0)).data.numpy()\n",
    "\n",
    "X_synthetic = np.vstack([fake_majority, \n",
    "                         fake_minority])\n",
    "y_synthetic = np.concatenate([np.zeros(majority_samples), \n",
    "                              np.ones(minority_samples)]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_org = DecisionTreeClassifier(max_depth=10) #LogisticRegression(solver='saga') \n",
    "clf_fake = DecisionTreeClassifier(max_depth=10) #LogisticRegression(solver='saga')\n",
    "\n",
    "predictive = {}\n",
    "predictive[\"real\"] = clf_org.fit(X=X_train, y=y_train_bin)\n",
    "predictive[\"synthetic\"] = clf_fake.fit(X=X_synthetic, y=y_synthetic)\n",
    "\n",
    "test_auc(predictive, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = {\"original\":[],\"GANbalanced\":[]}\n",
    "for i in range(200):\n",
    "    sample_size = X_minority.shape[0]*4\n",
    "    X_fake = generator(*generator.sample_latent(num_samples= sample_size, class_index=1)).data.numpy()\n",
    "    #X_fake = generator(generator.sample_latent(num_samples= sample_size, class_index=None)).data.numpy()\n",
    "    y_fake = np.ones(shape=[sample_size])\n",
    "\n",
    "    X_up = np.vstack([X_train,X_fake])\n",
    "    y_up = np.hstack([y_train_bin,y_fake])\n",
    "\n",
    "    clf_org = DecisionTreeClassifier(max_depth=5)\n",
    "    clf_fake = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "    upsampling = {}\n",
    "    upsampling[\"original\"] =  clf_org.fit(X=X_train, y=y_train_bin)\n",
    "    upsampling[\"GANbalanced\"] = clf_fake.fit(X=X_up, y=y_up)\n",
    "    \n",
    "    performance_temp = test_auc(upsampling, X_test, y_test_bin)\n",
    "    for model in performance_temp:\n",
    "        performance[model].append(performance_temp[model])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(performance).mean())\n",
    "print(pd.DataFrame(performance).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_function(X, y, clf, ax):\n",
    "    plot_step = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, alpha=0.4)\n",
    "    ax.scatter(X[:, 0], X[:, 1], alpha=0.8, c=y, edgecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
    "plot_decision_function(X_train, y_train, upsampling[\"original\"], ax1)\n",
    "plot_decision_function(X_up, y_up, upsampling[\"GANbalanced\"], ax2)\n",
    "\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [deeplearning]",
   "language": "python",
   "name": "Python [deeplearning]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
