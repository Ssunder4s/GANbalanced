{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation: One-group Data Generating Processes of Increasing Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as sp\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import minmax_scale, scale, MinMaxScaler\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, brier_score_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wgan.simulation import create_continuous_data\n",
    "from wgan.imblearn import GANbalancer\n",
    "import wgan.data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, ADASYN, SMOTENC\n",
    "from types import MethodType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import safe_indexing\n",
    "\n",
    "def create_samples(self, X, y):\n",
    "        # FIXME: uncomment in version 0.6\n",
    "        # self._validate_estimator()\n",
    "\n",
    "        for class_sample, n_samples in self.sampling_strategy_.items():\n",
    "            if n_samples == 0:\n",
    "                continue\n",
    "            target_class_indices = np.flatnonzero(y == class_sample)\n",
    "            X_class = safe_indexing(X, target_class_indices)\n",
    "\n",
    "            self.nn_k_.fit(X_class)\n",
    "            nns = self.nn_k_.kneighbors(X_class, return_distance=False)[:, 1:]\n",
    "            X_new, y_new = self._make_samples(X_class, y.dtype, class_sample,\n",
    "                                              X_class, nns, n_samples, 1.0)\n",
    "\n",
    "        return X_new, y_new\n",
    "\n",
    "\n",
    "SMOTE._sample_only = create_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artifical Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 5000\n",
    "N_VAR = 6\n",
    "data = {\n",
    "    \"Independent\" : create_continuous_data(n_samples=N_SAMPLES, n_var=N_VAR, n_dependent=0, pos_ratio=0),\n",
    "    \"Dependent\" : create_continuous_data(n_samples=N_SAMPLES, n_var=N_VAR, n_dependent=5, pos_ratio=0),\n",
    "    \"Mixed\" : create_continuous_data(n_samples=N_SAMPLES, n_var=N_VAR, n_dependent=N_VAR//2, pos_ratio=0)\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_balancer = GANbalancer(idx_cont=range(N_VAR), categorical=None, auxiliary=False,\n",
    "                           generator_input=N_VAR*1, generator_layers=None, \n",
    "                           critic_layers=[N_VAR, N_VAR],\n",
    "                          batch_size = 64, n_iter=25000, \n",
    "                           sampling_strategy = {0:N_SAMPLES, 1:0}, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(sampling_strategy = {0:N_SAMPLES, 1:0})\n",
    "smote._validate_estimator()\n",
    "smote.sampling_strategy_ = {0:N_SAMPLES, 1:0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_first_moments(X):\n",
    "    return np.vstack([x.mean(axis=0) for x in X])\n",
    "\n",
    "def check_second_moments(X):\n",
    "    return [np.cov(x, rowvar=False).round(2) for x in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Normal RVs - First and Second Moment Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25003it [05:43, 76.32it/s]                           \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GANbalancer(auxiliary=False, batch_size=64, categorical=None,\n",
       "      critic_iterations=5, critic_layers=[6, 6], generator_input=6,\n",
       "      generator_layers=None, idx_cont=range(0, 6),\n",
       "      learning_rate=(5e-05, 5e-05), n_iter=25000, random_state=None,\n",
       "      sampling_strategy={0: 5000, 1: 0}, verbose=1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gan_balancer._fit(data[\"Independent\"][0], y=np.zeros(shape=N_SAMPLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_balancer._update(data[\"Independent\"][0], y=np.zeros(shape=N_SAMPLES), n_iter=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.84621344,  0.03991291, -0.13927299,  0.02095798, -0.30758677,\n",
       "        -1.12091147],\n",
       "       [ 1.05729389, -0.61359793, -1.09567106,  0.02990071, -0.41301665,\n",
       "        -0.63037598],\n",
       "       [ 0.84651328,  0.04365111, -0.12803258,  0.0224761 , -0.30881362,\n",
       "        -1.10691709]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_first_moments([data[\"Independent\"][0], \n",
    "                     gan_balancer.generator.sample_data(N_SAMPLES),\n",
    "                     smote._sample(data[\"Independent\"][0], y=np.zeros(shape=N_SAMPLES))[0]\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(gan_balancer.generator.parameters())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.98,  0.01,  0.02, -0.01,  0.  ,  0.02],\n",
       "        [ 0.01,  0.98, -0.  , -0.01,  0.03, -0.01],\n",
       "        [ 0.02, -0.  ,  1.05, -0.  ,  0.  , -0.01],\n",
       "        [-0.01, -0.01, -0.  ,  0.95, -0.  , -0.01],\n",
       "        [ 0.  ,  0.03,  0.  , -0.  ,  1.  , -0.01],\n",
       "        [ 0.02, -0.01, -0.01, -0.01, -0.01,  1.01]]),\n",
       " array([[ 0.09,  0.01,  0.01,  0.02,  0.01, -0.01],\n",
       "        [ 0.01,  0.07, -0.  , -0.01,  0.01, -0.  ],\n",
       "        [ 0.01, -0.  ,  0.11,  0.02,  0.02, -0.02],\n",
       "        [ 0.02, -0.01,  0.02,  0.07,  0.01,  0.02],\n",
       "        [ 0.01,  0.01,  0.02,  0.01,  0.09, -0.01],\n",
       "        [-0.01, -0.  , -0.02,  0.02, -0.01,  0.1 ]]),\n",
       " array([[ 0.94,  0.01,  0.03, -0.02, -0.  ,  0.02],\n",
       "        [ 0.01,  0.94, -0.01, -0.02,  0.04, -0.  ],\n",
       "        [ 0.03, -0.01,  0.99, -0.01,  0.01, -0.01],\n",
       "        [-0.02, -0.02, -0.01,  0.89,  0.  , -0.01],\n",
       "        [-0.  ,  0.04,  0.01,  0.  ,  0.95, -0.01],\n",
       "        [ 0.02, -0.  , -0.01, -0.01, -0.01,  0.95]])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_second_moments([data[\"Independent\"][0], \n",
    "                     gan_balancer.generator.sample_data(N_SAMPLES),\n",
    "                     smote._sample(data[\"Independent\"][0], y=np.zeros(shape=N_SAMPLES))[0]\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(gan_balancer.generator.parameters())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Normal RVs - Covariance Approximation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_idx = \"Dependent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_balancer._fit(data[data_idx][0], y=np.zeros(shape=N_SAMPLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_first_moments([data[data_idx][0], \n",
    "                     gan_balancer.generator.sample_data(N_SAMPLES),\n",
    "                     smote._sample(data[data_idx][0], y=np.zeros(shape=N_SAMPLES))[0]\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covariance matrix extimate is very close to true covariance matrix, e.g. \n",
    "TRUE  ([[ 0.21, -0.11, -0.11, -0.14,  0.1 ],\n",
    "        [-0.11,  0.32,  0.08,  0.07, -0.23],\n",
    "        [-0.11,  0.08,  0.13,  0.11, -0.08],\n",
    "        [-0.14,  0.07,  0.11,  0.21, -0.07],\n",
    "        [ 0.1 , -0.23, -0.08, -0.07,  0.24]]),\n",
    "        \n",
    " GAN  ([[ 0.21, -0.09, -0.12, -0.17,  0.11],\n",
    "        [-0.09,  0.23,  0.07,  0.07, -0.14],\n",
    "        [-0.12,  0.07,  0.13,  0.13, -0.06],\n",
    "        [-0.17,  0.07,  0.13,  0.21, -0.06],\n",
    "        [ 0.11, -0.14, -0.06, -0.06,  0.2 ]]),\n",
    "        \n",
    "SMOTE ([[ 0.21, -0.11, -0.11, -0.14,  0.1 ],\n",
    "        [-0.11,  0.32,  0.08,  0.07, -0.23],\n",
    "        [-0.11,  0.08,  0.13,  0.11, -0.08],\n",
    "        [-0.14,  0.07,  0.11,  0.21, -0.07],\n",
    "        [ 0.1 , -0.23, -0.08, -0.07,  0.23]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_second_moments([data[data_idx][0], \n",
    "                     gan_balancer.generator.sample_data(N_SAMPLES),\n",
    "                     smote._sample(data[data_idx][0], y=np.zeros(shape=N_SAMPLES))[0]\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critic data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the critic to select samples that are more realistic from a larger set of generated observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = gan_balancer.generator(gan_balancer.generator.sample_latent(N_SAMPLES*20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_critic = np.argsort(\n",
    "    gan_balancer.critic(sample).detach().numpy().flatten()\n",
    "                         )\n",
    "idx_lowest = idx_critic[:N_SAMPLES].copy()\n",
    "idx_highest = idx_critic[-N_SAMPLES:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_lowest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Critic output is in the range $[-\\inf;0]$ where higher is less difference to the real distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_np = sample.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_lowest = sample_np[idx_lowest,:].copy()\n",
    "sample_random = sample_np[np.random.choice(range(0,N_SAMPLES), size=N_SAMPLES),:].copy()\n",
    "sample_highest = sample_np[idx_highest,:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_first_moments([data[\"Independent\"][0], \n",
    "                     sample_highest,\n",
    "                     sample_np,\n",
    "                     sample_lowest\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_second_moments([data[\"Independent\"][0], \n",
    "                     sample_highest,\n",
    "                     sample_np,\n",
    "                     sample_lowest\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = [(x,y) for x in range(no_vars) for y in range(no_vars) if y>x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=no_vars, ncols=no_vars, sharex=True, sharey=True, squeeze=True,figsize=(10,10))\n",
    "for y in axes:\n",
    "    for x in y:\n",
    "        x.set_xticklabels([])\n",
    "        x.set_yticklabels([])\n",
    "\n",
    "for i,j in combinations:\n",
    "    sns.kdeplot(X_majority[:,i], X_majority[:,j], alpha=0.5, cmap=\"Blues\", ax=axes[(j,i)])\n",
    "    sns.kdeplot(X_minority[:,i], X_minority[:,j], alpha=0.5, cmap=\"Greens\", ax=axes[(j,i)])\n",
    "fig.savefig(f'../img/cont_sample_tr_iter_{trainer.G.training_iterations}.png',format='png', dpi=100)\n",
    "    #fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 90\n",
    "\n",
    "for _ in range(30):\n",
    "    trainer.train(data_loader, epochs)\n",
    "    \n",
    "    \n",
    "    if modus == 'full':\n",
    "        fake_minority = generator(*generator.sample_latent(num_samples= 1000, class_index=1)).data.numpy()\n",
    "        fake_majority = generator(*generator.sample_latent(num_samples= 1000, class_index=0)).data.numpy()\n",
    "    elif modus == 'minority':\n",
    "        fake_minority = generator(generator.sample_latent(num_samples= 1000)).data.numpy()\n",
    "        \n",
    "    fig, axes = plt.subplots(nrows=no_vars, ncols=no_vars, sharex=True, squeeze=True,figsize=(10,10))\n",
    "    for y in axes:\n",
    "        for x in y:\n",
    "            x.set_xticklabels([])\n",
    "            x.set_yticklabels([])\n",
    "    \n",
    "    for i in range(no_vars):\n",
    "        sns.kdeplot(X_minority[:,i], alpha=0.5, shade=True, color=\"blue\", ax=axes[(i,i)])\n",
    "        sns.kdeplot(fake_minority[:,i], alpha=0.5, shade=True, color=\"green\", ax=axes[(i,i)])\n",
    "    \n",
    "    for i,j in combinations:\n",
    "        axes[(i,j)].set_ylim(0,1)\n",
    "        # majority (upper right)\n",
    "        if modus == 'full':\n",
    "            sns.kdeplot(X_majority[0:1000,i], X_majority[0:1000,j], alpha=0.5, cmap=\"Blues\", ax=axes[(i,j)])\n",
    "            sns.kdeplot(fake_majority[:,i], fake_majority[:,j], alpha=0.5, cmap=\"Greens\", ax=axes[(i,j)], )\n",
    "        \n",
    "        # minority (lower left)\n",
    "        sns.kdeplot(X_minority[:,i], X_minority[:,j], alpha=0.5, cmap=\"Blues\", ax=axes[(j,i)])\n",
    "        sns.kdeplot(fake_minority[:,i], fake_minority[:,j], alpha=0.5, cmap=\"Greens\", ax=axes[(j,i)])\n",
    "        \n",
    "    fig.savefig(f'../img/cont_sample_tr_iter_{trainer.G.training_iterations}.png',format='png', dpi=200)\n",
    "        #fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = f\"multinormal_n{N//1000}_k{no_vars}_{modus}\"\n",
    "torch.save(generator.state_dict(), f\"../models/wgan_generator_{desc}_{generator.training_iterations}\")\n",
    "torch.save(discriminator.state_dict(), f\"../models/wgan_discriminator_{desc}_{generator.training_iterations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"multinormal_n10_k4_c2_6999\"\n",
    "generator.load_state_dict(torch.load(f\"../models/wgan_generator_{file_name}\"))\n",
    "discriminator.load_state_dict(torch.load(f\"../models/wgan_discriminator_{file_name}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_minority = generator(*generator.sample_latent(num_samples= minority_samples, class_index=1)).data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(X_minority, axis=0))\n",
    "print(np.mean(fake_minority, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.quantile(X_minority, q=np.arange(0,1,0.1), axis=0))\n",
    "print(np.quantile(fake_minority, q=np.arange(0,1,0.1), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.cov(X_minority, rowvar=False) - np.cov(fake_minority,rowvar=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = X_minority.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = generator(*generator.sample_latent(num_samples= sample_size, class_index=1)).data.numpy()\n",
    "#fake = generator(generator.sample_latent(num_samples= sample_size)).data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fakereal = np.vstack([X_minority, \n",
    "                        fake])\n",
    "y_fakereal = np.concatenate([np.zeros(X_minority.shape[0]), \n",
    "                        np.ones(fake.shape[0])]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=50, min_samples_leaf=20, n_jobs=10)\n",
    "model_fakereal = clf.fit(X_fakereal, y_fakereal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_fakereal = model_fakereal.predict_proba(X_fakereal)[:,1]\n",
    "roc_auc_score(y_fakereal, pred_fakereal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive performance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_bin = np.argmax(y_train, axis=1)\n",
    "y_test_bin = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_auc(model_library, X, y_true):\n",
    "    auc = {}\n",
    "    for model in model_library.keys():\n",
    "        pred = model_library[model].predict_proba(X_test)[:,1]\n",
    "        auc[model] = roc_auc_score(y_true, pred)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minority_samples = X_minority.shape[0]\n",
    "majority_samples = X_majority.shape[0]\n",
    "\n",
    "fake_minority = generator(*generator.sample_latent(num_samples= minority_samples, class_index=1)).data.numpy()\n",
    "fake_majority = generator(*generator.sample_latent(num_samples= majority_samples, class_index=0)).data.numpy()\n",
    "\n",
    "X_synthetic = np.vstack([fake_majority, \n",
    "                         fake_minority])\n",
    "y_synthetic = np.concatenate([np.zeros(majority_samples), \n",
    "                              np.ones(minority_samples)]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_org = DecisionTreeClassifier(max_depth=10) #LogisticRegression(solver='saga') \n",
    "clf_fake = DecisionTreeClassifier(max_depth=10) #LogisticRegression(solver='saga')\n",
    "\n",
    "predictive = {}\n",
    "predictive[\"real\"] = clf_org.fit(X=X_train, y=y_train_bin)\n",
    "predictive[\"synthetic\"] = clf_fake.fit(X=X_synthetic, y=y_synthetic)\n",
    "\n",
    "test_auc(predictive, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = {\"original\":[],\"GANbalanced\":[]}\n",
    "for i in range(200):\n",
    "    sample_size = X_minority.shape[0]*4\n",
    "    X_fake = generator(*generator.sample_latent(num_samples= sample_size, class_index=1)).data.numpy()\n",
    "    #X_fake = generator(generator.sample_latent(num_samples= sample_size, class_index=None)).data.numpy()\n",
    "    y_fake = np.ones(shape=[sample_size])\n",
    "\n",
    "    X_up = np.vstack([X_train,X_fake])\n",
    "    y_up = np.hstack([y_train_bin,y_fake])\n",
    "\n",
    "    clf_org = DecisionTreeClassifier(max_depth=5)\n",
    "    clf_fake = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "    upsampling = {}\n",
    "    upsampling[\"original\"] =  clf_org.fit(X=X_train, y=y_train_bin)\n",
    "    upsampling[\"GANbalanced\"] = clf_fake.fit(X=X_up, y=y_up)\n",
    "    \n",
    "    performance_temp = test_auc(upsampling, X_test, y_test_bin)\n",
    "    for model in performance_temp:\n",
    "        performance[model].append(performance_temp[model])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(performance).mean())\n",
    "print(pd.DataFrame(performance).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_function(X, y, clf, ax):\n",
    "    plot_step = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, alpha=0.4)\n",
    "    ax.scatter(X[:, 0], X[:, 1], alpha=0.8, c=y, edgecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
    "plot_decision_function(X_train, y_train, upsampling[\"original\"], ax1)\n",
    "plot_decision_function(X_up, y_up, upsampling[\"GANbalanced\"], ax2)\n",
    "\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [deeplearning]",
   "language": "python",
   "name": "Python [deeplearning]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
